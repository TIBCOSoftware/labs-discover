/*
* Copyright Â© 2020. TIBCO Software Inc.
* This file is subject to the license terms contained
* in the license file that is distributed with this file.
*/
package com.tibco.labs.pm

import com.tibco.labs.utils.DataFrameUtils
import com.tibco.labs.utils.DataFrameUtils.{joinByColumn, joinByColumnGen}
import com.tibco.labs.utils.Status.sendBottleToTheSea
import com.tibco.labs.utils.commons._
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.LongType
import org.apache.spark.sql.{DataFrame, SparkSession}

import scala.util.{Failure, Success, Try}
object cases {

  import spark.implicits._
  def transformCases (df_events: DataFrame, df_events_casefilter: DataFrame): DataFrame = {
    var df_cases_f = spark.emptyDataFrame
Try {

  logger.info(s"###########  Creating variants ##########")

  logger.info(s"###########  Ordering Cases Dataframe ##########")
  //val df_cases_ordered = df_events.repartition(100,$"case_id").sortWithinPartitions("activity_start_timestamp", asc("row_id"))
  logger.info(s"###########  creating Cases Dataframe ##########")
  val _Start = System.nanoTime()
  val df_cases = df_events.repartition(100, $"case_id").withColumn("variant", collect_list("activity_id")
    .over(Window.partitionBy("case_id")
      .orderBy($"activity_start_timestamp".asc, $"row_id".asc)))
    .groupBy("case_id")
    .agg(min("activity_start_timestamp").as("case_start_timestamp"),
      max("activity_start_timestamp").as("case_end_timestamp"),
      sum("duration_sec").as("total_case_duration"),
      count("activity_id").as("activities_per_case"),
//      last("CASES_EXTRA_ATTRIBUTES").as("CASES_EXTRA_ATTRIBUTES"),
      max("variant").as("variants_cases"))
  val _End = System.nanoTime()
  val time = (_End - _Start) / 1000000
  logger.info(s"time for agg  : $time ms")
  logger.info(s"########### Cast ##########")
  val df_cases_0 = df_cases.withColumn("tmpVariants", concat_ws(",", $"variants_cases")).drop("variants_cases").withColumnRenamed("tmpVariants", "variants_cases")
  df_cases_f = df_cases_0
  //logger.info(s"########### join  ##########")
  //df_cases_f = broadcast(df_cases_0.as("cases")).join(df_variants.as("variants"), $"cases.variants_cases" === $"variants.variant")
  df_cases_f = df_cases_f.withColumn("analysis_id", lit(analysisId)) //.withColumn("idPK",sha2(concat_ws("||",col("VARIANT_ID"),col("analysis_id")),256))
  //val df_cases_finale_2 = df_cases_finale_1.withColumn("idPK", sha2(concat_ws("||", col("VARIANT_ID"), col("analysis_id"), col("case_id"), col("case_start_timestamp")), 256))
  // apply filters
  if(!df_events_casefilter.isEmpty) {
    logger.info(s"########### filtering cases table  ##########")
    df_cases_f = joinByColumnGen("case_id", df_events_casefilter, df_cases_f, "inner")
  } else {
    logger.info(s"########### no filtering cases table ##########")
  }


  df_cases_f = df_cases_f.withColumn("tmpStart", col("case_start_timestamp").cast("timestamp")).drop("case_start_timestamp").withColumnRenamed("tmpStart", "case_start_timestamp")

  df_cases_f = df_cases_f.withColumn("tmpEnd", col("case_end_timestamp").cast("timestamp")).drop("case_end_timestamp").withColumnRenamed("tmpEnd", "case_end_timestamp")

  df_cases_f = df_cases_f.withColumn("total_case_duration", when($"total_case_duration"=== "NULL", 0).otherwise($"total_case_duration"))
  df_cases_f = df_cases_f.withColumn("total_case_duration", when($"total_case_duration".isNull, 0).otherwise($"total_case_duration"))

  import org.apache.spark.ml.feature.Bucketizer

  //val minVal = df_cases_f.select(min("total_case_duration")).first.getLong(0).toDouble
  //val avgVal = df_cases_finale_4.select(avg("total_case_duration")).first.getDouble(0)
  //val medianVal = df_cases_finale_4.selectExpr("Double(approx_percentile(total_case_duration, 0.5, 100))").first.getDouble(0)
  val perctil15Val = df_cases_f.selectExpr("Double(approx_percentile(total_case_duration, 0.15))").first.getDouble(0)
  val perctil25Val = df_cases_f.selectExpr("Double(approx_percentile(total_case_duration, 0.25))").first.getDouble(0)
  val perctil50Val = df_cases_f.selectExpr("Double(approx_percentile(total_case_duration, 0.50))").first.getDouble(0)
  val perctil75Val = df_cases_f.selectExpr("Double(approx_percentile(total_case_duration, 0.75))").first.getDouble(0)
  val perctil95Val = df_cases_f.selectExpr("Double(approx_percentile(total_case_duration, 0.95))").first.getDouble(0)
  //val perctil99Val = df_cases_finale_4.selectExpr("Double(approx_percentile(total_case_duration, 0.99, 100))").first.getDouble(0)
  val maxVal = df_cases_f.select(max("total_case_duration")).first.getDouble(0)

  var splits = Array[Double]()
  splits = Array(Double.NegativeInfinity, 0.0, perctil15Val+0.01, perctil25Val+0.02, perctil50Val+0.03, perctil75Val+0.04, perctil95Val+0.05, maxVal+0.06, Double.PositiveInfinity)
  // sort array with 0 included, ordered ASC
  //splits = splits.toList.filter(_ >= 0).sortWith(_ < _).distinct.toArray
  splits = splits.toList.sortWith(_ < _).distinct.toArray

  val bucketizer = new Bucketizer()
    .setInputCol("total_case_duration")
    .setOutputCol("bucketedDuration")
    .setSplits(splits)
  // Transform original data into its bucket index.
  df_cases_f = bucketizer.transform(df_cases_f)

  val lengh_bck = bucketizer.getSplits.length - 1



  var buckets_labels: Map[Double, String] = null
  logger.info(s"Bucketizer output with $lengh_bck buckets")

  if (lengh_bck == 8) {
    buckets_labels = Map(
      -1.0 -> "Negative Values",
      0.0 -> "00th - 15th percentile",
      1.0 -> "15 - 25th percentile",
      2.0 -> "25th percentile - 50th percentile",
      3.0 -> "50th percentile} - 75th percentile",
      4.0 -> "75th percentile - 95th percentile",
      5.0 -> "95th percentile - 100Th Percentile",
      6.0 -> "Positive Infinity (beyond max)"
    )
  }
  val bucketL = buckets_labels.toSeq.toDF("bucketedDuration", "bucketedDuration_label")

  df_cases_f = df_cases_f.join(bucketL, Seq("bucketedDuration"))

  //df_cases_f = df_cases_f.drop("variant", "CASES_EXTRA_ATTRIBUTES")

  df_cases_f = df_cases_f.withColumn("temp_duration", round(col("total_case_duration")).cast(LongType))
    .drop("total_case_duration")
    .withColumnRenamed("temp_duration","total_case_duration")





/*
  val finalColumnsOrderedName: Array[String] = Array(
//    "variant",
    "variant_id",
    "case_id",
    "case_start_timestamp",
    "case_end_timestamp",
    "total_case_duration",
    "activities_per_case",
//    "CASES_EXTRA_ATTRIBUTES",
    "analysis_id",
    "bucketedDuration",
    "bucketedDuration_label",
    "caseStatus",
    "caseStatusCategory")



  df_cases_f = df_cases_f.select(finalColumnsOrderedName.head, finalColumnsOrderedName.tail: _*)
*/

/*  // Delete any previous analysisID in DB
  DataFrameUtils.deleteAnalysisJDBC(databaseName, "cases")
  //Write Events table
  if (backEndType.equals("aurora")) {
    DataFrameUtils.writeAnalysisJDBC(df_cases_f, databaseName, "cases")
  }*/
}match{
  case Success(_) => df_cases_f
  case Failure(e) => logger.error("Error in cases : " +e.getMessage)
    //sendTCMMessage(s"$analysisId", s"$caseRef", "error", s"${e.getMessage}", 0, databaseName, LocalDateTime.now().toString)
    sendBottleToTheSea(s"$analysisId","Error in cases",e.getMessage,0, organisation)
    spark.emptyDataFrame
}
  }


  def enrichCaseTable(df_cases: DataFrame, df_variants: DataFrame, df_activities: DataFrame): DataFrame = {
    var df_cases_f = spark.emptyDataFrame
    Try {
      logger.info(s"########### join  ##########")
      df_cases_f = broadcast(df_cases.as("cases"))
        .join(df_variants.as("variants"), $"cases.variants_cases" === $"variants.variant")
        .select("cases.*", "variants.variant_id")


      // classify cases if needed
      // add 2 new cols

      if(startFilters.nonEmpty || stopFilters.nonEmpty) {
        logger.info("Classification of cases...")
        df_cases_f =  new casesClassifier().classify(df_cases_f, startFilters, stopFilters, df_activities)
      } else {
        df_cases_f = df_cases_f.withColumn("caseStatus", lit("Undefined")).withColumn("caseStatusCategory", lit("Active"))
      }

      val finalColumnsOrderedName: Array[String] = Array(
        //    "variant",
        "variant_id",
        "case_id",
        "case_start_timestamp",
        "case_end_timestamp",
        "total_case_duration",
        "activities_per_case",
        //    "CASES_EXTRA_ATTRIBUTES",
        "analysis_id",
        "bucketedDuration",
        "bucketedDuration_label",
        "caseStatus",
        "caseStatusCategory")



      df_cases_f = df_cases_f.select(finalColumnsOrderedName.head, finalColumnsOrderedName.tail: _*)


      /*  // Delete any previous analysisID in DB
        DataFrameUtils.deleteAnalysisJDBC(databaseName, "cases")
        //Write Events table
        if (backEndType.equals("aurora")) {
          DataFrameUtils.writeAnalysisJDBC(df_cases_f, databaseName, "cases")
        }*/
    }match{
      case Success(_) => df_cases_f
      case Failure(e) => logger.error("Error in cases : " +e.getMessage)
        //sendTCMMessage(s"$analysisId", s"$caseRef", "error", s"${e.getMessage}", 0, databaseName, LocalDateTime.now().toString)
        sendBottleToTheSea(s"$analysisId","Error in cases",e.getMessage,0, organisation)
        spark.emptyDataFrame
    }
  }



  def doubleToStringTimeSpan(input: Int): String = {
    val week_r: Int = input % 604800
    val week: Int = (input - week_r) / 604800
    val day_r: Int = week_r % 86400
    val day: Int = (week_r - day_r) / 86400
    val hour_r: Int = day_r % 3600
    val hour: Int = (day_r - hour_r) / 3600
    val minute_r: Int = hour_r % 60
    val minute: Int = (hour_r - minute_r) / 60
    val sec: Int = minute_r % 60
    s"$week W $day D $hour:$minute:$sec"
  }

}
